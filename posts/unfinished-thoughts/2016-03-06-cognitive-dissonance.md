---
title: Artificial Cognitive Dissonance
author: Silviu Pitis
---

Can a machine experience cognitive dissonance in the same way humans do?

Cognitive dissonance refers to the strong negative feeling we experience when we hold two inconsistent views, which forces us to either (1) change one of the views, (2) give up confidence in the two views, or (3) find a differentiating feature.

Interestingly, we don't usually experience cognitive dissonance when faced with a "training example" that we get wrong. If we cannot come up with an explanation or the training example is not seen as very important, we simply reject the example. In some cases, our rejection is very strong, and can actually cause us to believe the opposite.

Rejection of training examples is something that backpropagating neural networks don't do. Backpropagation alone always leads to the network giving up confidence in the two competing views. This is often not the best strategy.

Is there a way to program a machine to behave like humans when faced with inconsistent beliefs?
