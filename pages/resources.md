---
title: Resources
---

# Resources

Below are links to the resources that I have read, understood, and liked enough to recommend. This list is limited in scope by my current understanding.

### Introductory

[Andrew Ng's machine learning course](https://www.coursera.org/learn/machine-learning) is the goto recommendation. You can also catch Andrew's full length lectures for Stanford's CS229 [here](https://youtu.be/UzxYlbK2c7E?list=PLA89DCFA6ADACE599). If you're searching for ideas, be sure to check out the "recent years' projects" on the [CS229 course site](http://cs229.stanford.edu/).

### Neural Networks and Backpropagation

_Module 1: Neural Networks_ of Stanford's [CS231n](http://cs231n.github.io/) is an excellent introduction to neural networks and backpropagation.

On backpropagation specifically, [Colah's backprop post](http://colah.github.io/posts/2015-08-Backprop/) provides a good overview, and [Andrej Karpathy's in-depth backpropagation guide](http://karpathy.github.io/neuralnets/) is the cleanest I've come across. Make sure you understand every line of [A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/).

### Convolutional Neural Networks (CNNs)

CNNs are not that different from fully-connected neural networks. _Module 2: Neural Networks_ of Stanford's [CS231n](http://cs231n.github.io/) is perfect.

### Recurrent Neural Networks (RNNs)

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a fun article by Andrej Karpathy that provides an entertaining introduction to RNNs.

If you want to actually understand how an RNN is coded though, I would refer you to this [excellent tutorial series](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) by Denny Britz. The critical piece is [part 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), which discusses backpropagation through time (BPTT).

For LTSMs (a type of beefed up RNN), Denny links to [Colah's article on LTSMs](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), which provides a clear explanation of what goes into an LTSM module.
